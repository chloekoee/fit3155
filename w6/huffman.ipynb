{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Encoding\n",
    "1. Initalise all unique ASCII characters as Huffman nodes\n",
    "2. Hold a priority queue/min heap holding all elements with their key being their frequency\n",
    "3. While the min heap has a size of greater than 1\n",
    "   1. Pop the top two elements out\n",
    "   2. Create a new huffman node from these elements \n",
    "   3. Add this node to the min heap\n",
    "4. Do a DFS of the remaining Huffman node, returning a dictionary of each leaf node symbol: recursive stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D': '00', '_': '01', 'A': '10', 'E': '110', 'C': '1110', 'B': '1111'}\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from bitarray import bitarray\n",
    "## Finish type hings\n",
    "class HuffmanNode():\n",
    "    def __init__(self, value: str = None, frequency: int = 0, right  = None, left = None):\n",
    "        self.value = value\n",
    "        self.frequency = frequency\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        \"\"\"Compare nodes based on frequency for heapq.\"\"\"\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        #return f\" Value: {self.value} Frequency: {self.frequency}\"\n",
    "        return f\" Value: {self.value} \\nFrequency: {self.frequency}\\nRight: {self.right}\\nLeft: {self.left}\"\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_codes(current, codes, path : str):\n",
    "    '''Conducts DFS on a HuffmanNode returning the stack (binary encoding) and leaf value\n",
    "    As no cycles exist in a tree, no need for a visited data structure'''\n",
    "    ## base case have hit a leaf\n",
    "    if not current.right: ## can just check for right child, as all nodes will either have no children or both left and right\n",
    "        codes[current.value] = path\n",
    "        return codes\n",
    "    \n",
    "    ## traverse the children \n",
    "    codes = retrieve_codes(current = current.left, codes = codes, path = path + \"0\")\n",
    "    codes = retrieve_codes(current = current.right, codes = codes, path = path + \"1\")\n",
    "    return codes\n",
    "\n",
    "    \n",
    "    \n",
    "def huffman_encode(str:str):\n",
    "    if len(str) == 0:\n",
    "        return\n",
    "    # Create a hashmap in order to find all unique ASCII characters in str, ensure each \n",
    "    # new character has a new Huffman node\n",
    "    chars = defaultdict(lambda: HuffmanNode(value = None)) ## Maps characters to huffman nodes\n",
    "\n",
    "    for c in str:\n",
    "        if chars[c].value is None:\n",
    "            chars[c].value = c\n",
    "        chars[c].frequency += 1 ## Increment the frequency of the given entry \n",
    "\n",
    "    min_freq = list(chars.values())\n",
    "    heapq.heapify(min_freq)\n",
    "\n",
    "    ## Coalesce elements until only one remains - the root node\n",
    "    while len(min_freq) > 1: ## potentially inefficient o(n)\n",
    "        ## pop off two least frequent elements\n",
    "        least = heapq.heappop(min_freq)\n",
    "        second = heapq.heappop(min_freq)\n",
    "\n",
    "        name = least.value + second.value\n",
    "        freq = least.frequency + second.frequency\n",
    "        combined = HuffmanNode(value = name, frequency = freq, left = least, right = second)\n",
    "\n",
    "        heapq.heappush(min_freq, combined)\n",
    "\n",
    "    root = min_freq.pop()\n",
    "\n",
    "    ## potentially can make a predecessor array to backtrack\n",
    "    codes = defaultdict(lambda: \"\")\n",
    "    ## perfrom dfs from the root node to return all character encodings\n",
    "    return dict(retrieve_codes(current = root, codes = codes, path = \"\"))\n",
    "\n",
    "print(huffman_encode(\"A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitarray()\n"
     ]
    }
   ],
   "source": [
    "chol = bitarray()\n",
    "\n",
    "print(chol + bitarray(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elias Gamma Encoding\n",
    "##### Encoding\n",
    "1. Find the largest $N$ with $2^n = x$ (greatest power of $2$)\n",
    "2. Encoding $N$ using unary coding (with zeros) \n",
    "3. Append the integer $x - 2^N$ using $N$ digits in binary\n",
    "\n",
    "> Example : Encoding $10$\n",
    "> 1. $2^3$ = $8$ \n",
    "> 2. $N = 3$. $3$ in unary encoding = `000`, add one to obtain `0001`\n",
    "> 3. $10-8 = 2$, therefore encoding $2$ in binary: `010`\n",
    "> Therefore $10$ = `0001010`\n",
    "\n",
    "##### Decoding\n",
    "1. Decode unary code at the beginning (count first zeroes) this is our $N$\n",
    "2. Read the next $N+1$ bits as a binary number, that corresponds to $x-2^N$\n",
    "\n",
    " Example : Decoding `num =` `0001010`\n",
    "> 1. Unary code is all zeros before first 1 : `000`, equals 3. $\\therefore N = 3$\n",
    "> 2. `num[N+1:]` is `010`. This represents $2$\n",
    "> 3. Therefore `num` = $2^N + 2 = 2^3 +2 = 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elias Omega Encoding\n",
    "Balances the sizes of very different integers. Use case: when need to store input data in a very large integer range. \n",
    "Will represent integers closer to 0 and very far from 0 with a more balanced/equal allocation of space. \n",
    "##### Encoding:\n",
    "$L_n...L_3, L_2, L_1, N$ \n",
    "Calculated recursively. Continues until $|L_i| = 1$. Initial case is $L1 = |N|$ (number of bits in $N$)\n",
    "1. Calculate $L_i$ = $|L_{i-1}|$ with the leading bit flipped from $1$ to $0$\n",
    "2. Prepend $L_i$ to the encoding / recursively add innermost stack call return to gradually outer stack call returns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00100011000110001\n"
     ]
    }
   ],
   "source": [
    "import typing \n",
    "from bitarray import bitarray\n",
    "\n",
    "def elias(n: int):\n",
    "    '''\n",
    "    elias returns elias encoding for a given decimal integer\n",
    "    '''\n",
    "    ## recursive function\n",
    "    def add_code(l: bitarray) -> bitarray:\n",
    "        ## If the length component is equal to 1, return \n",
    "        if len(l) == 1:\n",
    "            return l\n",
    "        \n",
    "        ## Recursive case, calculate the next smallest length component to be prepended\n",
    "        l_length : int = (len(l)-1) \n",
    "\n",
    "        ## Convert it to binary and flip the leading bit\n",
    "        next_l : bitarray  = bitarray(bin(l_length)[2:])\n",
    "        next_l[0] = 0\n",
    "        return add_code(next_l) + l\n",
    "    \n",
    "    ## Convert n to binary (removing '0f' prefix)\n",
    "    n_binary : bitarray = bitarray(bin(n)[2:])\n",
    "    return add_code(n_binary).to01() ## Converts it to string form\n",
    "\n",
    "print(elias(561))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Decoding\n",
    "\n",
    "Starting `component` = `encoded[0]`.\n",
    "1. Read component. If it `component[0]` is $0$, this is a length component. \n",
    "   1. Switch the first bit such that `component[0]` = 1\n",
    "   2. Read component. Value of component = `|next_component|` + 1 \n",
    "   3. Shift to `next_component` which exists at the position after `component` `i` to `i + |next_component|`\n",
    "   4. Continue \n",
    "2. If component[0] = 1 this indicates that this component is $N$ so can read $N$.\n",
    "\n",
    "Elias Omega Encoding guarantees that each number represented has a unique prefix - as it recursively encodes each order of magnitude. \n",
    "Idea of mapping from the positive integer set to the countable integer set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Correct!\n"
     ]
    }
   ],
   "source": [
    "import typing \n",
    "from bitarray import bitarray\n",
    "\n",
    "def decode_elias(code: str):\n",
    "    '''\n",
    "    decode_elias recursively and efficiently decodes a binary string encoded in elias omega\n",
    "    '''\n",
    "    ## convert code to bitarray\n",
    "    code = bitarray(code)\n",
    "\n",
    "    ## recursive function\n",
    "    def return_start_n(i: int, length = int) -> int:\n",
    "        ## If the first bit is 1, i points to the start of N\n",
    "        if code[i] == 1:\n",
    "            return i\n",
    "        \n",
    "        ## Else if the first bit is a 0, this is a length component to traverse over\n",
    "        code[i] = 1 # Flip the first bit of this length component to 1\n",
    "        next_length = int((code[i : i + length]).to01(),2) # Calculate the length of the following length component\n",
    "        next_length += 1 # Add one to offset the -1 done in the encoding\n",
    "\n",
    "        ## Calculate the next position of i\n",
    "        next_i = i + length # After having processed this length component, shift i rightwards\n",
    "        return return_start_n(i = next_i, length = next_length)\n",
    "    \n",
    "    ## first component is always a length component = 0 with size = 1\n",
    "    start = return_start_n(i = 0, length = 1) \n",
    "    return int(code[start:].to01(),2) #code[start:] \n",
    "\n",
    "\n",
    "for i in range(1,10000):\n",
    "    # print(i)\n",
    "    assert i == decode_elias(elias(i))\n",
    "print(\"All Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LZ77 Compression\n",
    "Window comprises of two consecutive parts\n",
    "1. Search window ('dictionary')\n",
    "2. Lookahead buffer ('buffer)\n",
    "\n",
    "To encode any substring in the lookahead buffer, find the largest matched substring in the dictionary\n",
    "1. The offset (distance of the match from the start position of the match from the current substring being encoded)\n",
    "2. Lenght of the match\n",
    "3. The next character `char` in the lookahead buffer `(k+1)`\n",
    "\n",
    "Using this search, the substring at the current position is encoded as a triple `<offset, length, char>` \n",
    "\n",
    "Note that this triple is not stored as `<integer, integer, character>`\n",
    "\n",
    "It is stored as variable length bits - `<elias encoding, elias encoding, huffman encoding>`\n",
    "\n",
    "Everything time shift `j` up (`j` is where we are currently looking within the lookahead buffer) we ask the question \n",
    "> what is the longest substring starting at that position that matches in the past?\n",
    "\n",
    "- Let `b` = size of lookahead buffer\n",
    "- Let `s` = size of search window \n",
    "The size of the lookahead buffer, call it `b` is a parameter, setting it to greater values will trade speed for space (takes far longer to evaluate matches, as they are truncated at a longer length, but resulting compression will take up much less space). Thus the lookahead buffer bounds the length of the match found between `str[j....j+b]`.\n",
    "This match can occur between `str[j-s...j-s+b]`, so only matches of up to size `b` are compressed\n",
    "\n",
    "```python\n",
    "triple = [offset = 0, len = 0, next_char = str[0]]\n",
    "1. While j < n\n",
    "    1. Find the longest matched substring starting after search window and matching j as much as possible up until j+b\n",
    "        1. If one exists, update triple\n",
    "   1. j = j + offset ## move past the largest match made at j \n",
    "\n",
    "```\n",
    "\n",
    "What remains after the LZ77 Encoding is an array, or some collection of triples\n",
    "\n",
    "\n",
    "In order to further optimise, LZSS, uses a tuple `<bit-flag = bit-0, current_char>`  \n",
    "\n",
    "rather than a triple `<bit-flag = bit-0, offset, length>` \n",
    "\n",
    "for all characters which have no previous match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder LZ77\n",
    "Given a string `str = ababab$`\n",
    "And a search window of a given size `sw`\n",
    "The resultant triples encoding `str` would be\n",
    "1. `<0,0,'a'>`\n",
    "2. `<0,0,'b'>`\n",
    "3. `<2,4,'$'>` \n",
    "\n",
    "> Note that the triples always encode one letter greater than what we read\n",
    "Each triple will be encoded as\n",
    "`<Elias-Start, Elias-Length, Huffman-Character>`\n",
    "\n",
    "And the huffman codes in the preamble will be encoded as\n",
    "`<ASCII-Code, Elias-Length, Huffman-Code-Character>`\n",
    "Where each of these triples describes the huffman code for a given character (in lexicographical order, doesn't matter)\n",
    "Where ASCII is always one byte, elias-length is self-encoded/contained, huffman-character is variable length\n",
    "Note here that the elias-length is the length of the huffman code for this given character\n",
    "This will be read by the decoder and each triple inserted into a hashmap for quick look up/decoding\n",
    "\n",
    "If the decoder will construct the huffman encodings itself, rather than encoding it in the encoder step, the preamble constituents instead can be \n",
    "`<ASCII-Code, Elias-Frequency>`\n",
    "Where Elias-Frequency describes the frequency of the given ASCII character\n",
    "\n",
    "The resultant output file for this encoding will contain:\n",
    "```python\n",
    "1. Elias encoding for the length of the preamble (so decoder knows where to read up to)\n",
    "2. Preamble triples <ASCII, EliasLength, HuffmanCode>\n",
    "3. LZ77 Triples <EliasStart, EliasLength, HuffmanCode>\n",
    "```\n",
    "\n",
    "Encoder Function: \n",
    "Parameters: \n",
    "- `sw` which is the search window size\n",
    "- `bs` which is the buffer size\n",
    "\n",
    "Pointers in Iteration:\n",
    "1. `i`: one in the str context holding the current position of str that we are encoding\n",
    "2. `b`: and one starting at the buffer window (this is calculated as `i - bs`)\n",
    "3. (tentative, implicit) `s`: `i + sw`\n",
    "\n",
    "Idea is that we want the maximal match/the greatest matched prefix from `str[i...s]`\n",
    "that matches something occuring after `str[b]` (can this occur all the way up to `[i...s]`?)\n",
    "\n",
    "potential optimisations i thought about but failed\n",
    "1. calculating the huffman codes from the the triples rather than from the string\n",
    "not space efficient as have to store all the triples associated with the entire document (e.g. all 100000 pages) to calculate the huffman codes for just the characters afterwards. \n",
    "2. only including the huffman encoding of characters occuring in the triples in the preamble, rather than all huffman encodings\n",
    "every character in the string will occur as at least ONCE in the sequence of triples (as for the first occurence of that character, nothing to match to occuring before it) \n",
    "\n",
    "```python\n",
    "## calculate all huffman codes from the original string\n",
    "huffman_codes : hashmap = huffman(str)\n",
    "encoding = bitarray()\n",
    "## Constructing LZ77 Triples\n",
    "while i < len(str):\n",
    "    buffer = str[b...i]\n",
    "    search = str[i...s]\n",
    "    use modified z algorithm on the compound string : search + \"$\" + buffer + search\n",
    "        find the greatest z value occuring within the buffer portion of this compound string\n",
    "        if there a multiple matching values, tie breaker is the leftmost one, as minimises elias encoding for offset.add(str[i])\n",
    "    new triple = <offset (position of chosen z value), maximum z value, huffman_codes[str[i]]>\n",
    "    encoding.append(binary(new_triple)) ## add binary value of new triple to encoding\n",
    "    i += length\n",
    "\n",
    "## Constructing the preamble\n",
    "preamble_length = 0\n",
    "For each character, code in huffman_codes:\n",
    "    elias_length = elias(len(code))\n",
    "    ascii_code = ascii(character)\n",
    "    triple = <ascii_code, elias_length, code> ## encode this as a bitarray\n",
    "\n",
    "    preamble_length += length(triple)\n",
    "    encoding.prepend(triple)\n",
    "\n",
    "## Prepend the length of the preamble\n",
    "preamble_length  = binary(preamble_length) \n",
    "preamble_length = preamble_length + length(preamble_length) ## offset by the length of preamble length\n",
    "encoding.prepend(preamble_length)\n",
    "\n",
    "## Construct the encoding \n",
    "return encoding ## or write encoding to a text file\n",
    "```\n",
    "Research into how bitarrays are stored - if they are linked list, prepending and appending are efficient\n",
    "Potential optimisation - instead of doing a naive scan for the first and second character, know the first triple is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional AND FEASIBLE optimisation:\n",
    "``` python\n",
    "last_seen = [0 for i in range (ascii range)]\n",
    "1. Create a character indexed (last seen) array, which holds the last given occurence for a given character. When we encounter a new character, check if it has occured within the search window:\n",
    "if last_occurrence[character] is within i - buffer size e.g     [i-b]    occurs here   [i]\n",
    "then need to do z box\n",
    "however, if last occurrence is not within current sliding window, then skip z algorithm and instead\n",
    "immediately insert the triple <0,0,character>\n",
    "\n",
    "This prevents unnecessary z algorithm being done\n",
    "```\n",
    "\n",
    "May not actually do anything given small alphabet size (e.g. with DNA), large search window (based on hardware)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
